## Notes for Developer associate
- The terms of availability Zone, the customer Data is cashed before you access it.
- IAM basics
    - control of your AWS account.
    - shared access to your AWS account.
    - permissions.
    - Identity Federation.
    - IAM allows multi-factor Authentication and this allows - increase in access and resources.
    - IAM
        - Temporary Access.
        - password policies.
        - it integrated.
        - Compliance it supports PCI DSS.
    - Idenity Access Management
        - user
        - groups
        - roles
    - We also have to have to setup password policies in terms of requirements, we can also setup password expirations on the configure.
    - We have IAM roles which AWS identity with permission policies that determine what the identity can and cannot do in AWS.
    - Testing IAM permisions
        - IAM Policy simulator
        - you can validate the policy if it works.
-  Elastic Compute Cloud(EC2)
    - it running a virtual machines in the cloud.
    - EC2 you only pay for what you use.
    - The is no wasted capacity and you can grow and shrink when you need.
- Pricing models for EC2
    - On demand, you pay by the hour or seconds depending on the type of the instances.
    - Reserved instances capacity may be 1 or 3 years and you can get 72% discuont and this is reginal.
    - Spot is purchase unused capacity at a discount up to 90% and this prices fluctuate.
    - Dedicated is a physical EC2 server which is on prem.
        - On demand we lloking at:
            - Flexible workloads
            - Short-term Applications.
            - Testing the Application or the Cloud service.
        - Reserved Instances
            - Predictable usage, applications with steady state.
            - specific Capacity requirements applications that require reserved capacity.
            - Pay up-front this payment reduce thier total computing costs.
        - Types of reserved instances
            - standard is up to 72% off demand price.
            - covertable up to 54% on demand.
            - Scheduled.
        - Spot instances
            - flexible start and end.
            - low compute.
            - an urgent need for large compute.
- EC2 Instance Types
    - Hardware.
    - Capabilities different types of compute.
    - Application requirements 
    - more infor
        - General purpose instances provide a balance of compute, memory and networking resources, and can be used for a variety of diverse workloads. 
        - Compute Optimized instances are ideal for compute bound applications that benefit from high performance processors. 
        - Memory optimized instances are designed to deliver fast performance for workloads that process large data sets in memory.
        - Accelerated computing instances use hardware accelerators, or co-processors, to perform functions, such as floating point number calculations, graphics processing, or data pattern matching, more efficiently than is possible in software running on CPUs.
        - Storage optimized instances are designed for workloads that require high, sequential read and write access to very large data sets on local storage.
        - The is a monitor for your Ec2 instance which is CloudWatch by default.
        - You can also add a bootstrap script in your Ec2 instance.
        - When you want to ssh into your instance you have to run ec2-user@(Ip address) -i and the key pair.
        - in this case we will have to start apache server by running systemctl starthttpd which will start the web server but allow auto on the server we will run systemctl enable httpd.
        - to check if everything is running, systemctl status httpd
    - EBS Volumes 
        - ebs is elastic block store, it is a storage you can attach to an EC2 instances.
        - you can used for run a database, run OS, store Data and install applications.
        - Mission critical
            - production workloads.
            - Highly available it replicates in a signle availability zone to protect against system failure.
            - Scable this mean it can is dynmically increase capacity and change the type volume.
        - EBS Volume types - Solid state disk(SSD)
            - General purpose SSD (gp2), we run 3 IOPS per GIB, up to maximum of 16 000 IOPS per volume, the gp2 volumes smaller than 1TB can burst up to 3 000 IOPS.
            - General purpose SSD (gp3), it is the latest generation the base line is 3 000 IOPS for any volume size (1GB - 16TB) they deliver up 16 000 IOPS and they 20% cheaper than gp2.
            - they both not latency sensitive.
        - Provisoned IOPS SSD(io1)
            - is the most expensive option on AWS.
            - you get 64 000 IOPS per volume 50 IOPS.
            - This is designed for I/O intensive applications, large databases and lantency-sensitive workloads.
        - Provisoned IOPS SSD(io2)
            - lasted generation.
            - Higher durability and more IOPS.
            - it is the same price as io1.
            - you get 500 IOPS per GIB and up to 64 000 IOPS
            - it is 99.9999% durable.
        - Provisoned IOPS SSD(io2 Block Express)
            - SAN(Storage Area Network) in the cloud, highest performance, sub-millisecond latency.
            - this uses EBS block express architecture.
            - 4x throughput, IOPS and capacity of regular io2 volumes
            - up to 64TB, 256,000 IOPS per volume and 99.999% durable.
            - This is for large, most critical high performance applications like SAP HANA, Oracle, Microsoft SQL Server and IBM DB2.
        - Throughput Optimized HHD(st1)
            - low-cost HHD volume.
            - baseline of 40MB/s per TB and the ability to burst up to 250 MB/s TB
            - maximum throughput of 500 mb/s per volume.
            - frequently-accessed throughput intensive workloads.
        - IOPS vs Throughput
            - IOPS
                - it measure the number of read and write operation per second.
                - important metric for quick transactions , low latency apps.
                - The ability to action read and write very quickly.
                - option to choose io1 or io2.
            - Throughput
                - it measure the number of bits read and write operation per second (MB/s).
                - important metric of large Datasets, complex queries
                - The ability to deal with large datasets.
        - setup an RDS volume on your EC2 instance
            - When creating a volume it needs to be in the same availability zone.
        - Elastic Load Balancer
            - A load balancer distributes network traffic across a group of serevers.
            - it allows us to increase traffic if the is alot of workload on the traffic.
            - Types of Elastic Load Balancer
                - Application load balancer, HTTP and HTTPS.
                - Network load balancer, TCP and high performance.
                - Classic load balancer http/https and tcp.(This is the legacy option)
            - Gateway Load Balancer
                - This allows you to load balanc workloads for third-party virtual applications running on AWS.
                - virtual application purchased using AWS market place.
                - virtual firewalls from companies such as Fortinet, Palo Alto, Cisco.
            - X-forwarded-for header
                - The X-Forwarded-For request header is automatically added and helps you identify the IP address of a client when you use an HTTP or HTTPS load balancer. Because load balancers intercept traffic between clients and servers, your server access logs contain only the IP address of the load balancer.
                -  504 Error Gateway time out it means the application on the downstream failed to connect.
            - Route 53 is an Amazon DNS service
                - The steps to configure Route53.
                - we will launch an EC2 instance and install httpd and create a application load balancer and finally configure a Route53 and map a domain name to our application load balancer to access the website.
                - IF we want to look at our DNS records w e go to Hosted Zones.
                - After we done creating our EC2 we have to go and create a load balancer.
                - if we want to change the name of our domain we have to create a new record in this case we have to create an alias.
            - We looking at the CLI demo for EC2 instance
                - we have to ssh ec2-user@ip address -i (name of key pair)
                - to setup your AWS configure you need an access key of your user from IAM user and a secret key from your user as well to able to access aws features in your cli.
                - you also have to include a default region which may be us-east-1 or another availability zone.
                - you can also view your aws configure script by list.
                - For good practice always remeber for least Privilege which states that give your users the minimum amount of access required to do they job.
                - use Groups
                    - create IAM groups and assign your users to groups.
                    - goup permissions are assigned using IAM policy document.
            - Launching EC2 with S3 roles
                - We have to create the IAM role and create the EC2 and configure that to access our S3 Bucket.
                - by assigning a role to access an S3 bucket to your EC2 you don't need to create a user and configure a user for AWS configure file in your EC2 instance.
                - exam tips
                    - IAM roles are preferred option from a secuirty perspective.
                    - You should avoid hard coding your credentials in your EC2 instance.
                    - you can update a policy attached to a role.
        - Relational Database Service(RDS)
            -  is a web service that makes it easier to set up, operate, and scale a relational database in the AWS Cloud.
            - Data is organized into tables
            - we also have rows and columns.
            - RDS is normaly used for online transaction processing workloads like Accounts for Customers.(OLTP) adn also Online Analytics processing(OLAP).
            - RDS types
                - SQl server
                - Oracle
                - MySQL
                - PostgreSQL
                - MariaDB
                - AUrora
            - RDS can in matters of minutes, this include Multi-AZ, Failover capability and Automated backups.
            - OLTP and OLAP
                - OLTP
                    - This allows us to process data from transactions in real-time, customer, orders and baking trasactions.
                - OLAP
                    - This allows us to process complex queries to analyze historical data like net profit figures from the past 3 years or sales forecasting.
            - Demo for RDS
                - Launch an RDS instance, launch EC2 instance and install MySQL database
                - connect the RDS instance using our EC2 instance.
            - When  creating an RDS service you can't change the VPC.
            - you can run a script under user data when creating your EC2 instance
                ~~~
                #!/bin/bash
                yum upadte -y
                yum install mysql -y
                ~~~
            - for our EC2 instance to communicate with RDS we will have to configure the Security policy groups for RDS.
            - We going to configure our inbound rules on EC2 Security groups.
            - Multi-AZ and read Replicas
                - It an exact copy of your production database in another availability Zone.
                - In this case we have primary us-east-1a and a standby Database us-east-1b.
                - In terms of the the solutiion AWS handles everything ffor you.
                - In this case when you write to your production DB this will automatically synchronize to the standby DB.
                - Types of RDS can be Multi-AZ
                    - SQL Server.
                    - Oracle.
                    - MySQL.
                    - PostgreSQL
                    - MariaDB
                - RDS failover will go the standby DB if the is a failure with the primary DB.
                - Multi-AZ is for disaster recovery option.
                - to improve performance of your RDS is to add read replicas.
                - it is a read-only copy of your primary database.
                - A read reeplica can be located in the same availability zone.
                - it can also be cross AZ or even cross region.
                - each replica has it's own DNS endpoint they're independent.
                - they can be promoted to be they own DBs.
                - key facts about read replicas
                    - scaling read performance.
                    - require autyomatic backup.
                    - multiple read replicas are supported.
                        - MySQL.
                        - MariaDB.
                        - PostgreSQL.
                        - Orcale.
                        - SQL Server.
                - you are allowed to add up to 5 read replicas.
            - RDS backups and snapshots
                - database snapshots
                    - manual, user initiated.
                - aautomated backup
                    - enabled by default, which creates daily backups.
                    - point-in time recovery which allows you to recover at any time.
                    - fully daily backup and the transactions logs are stored as well.
                    - The recovery process in this case AWS will choose the most recent snapshop.
                - The automated backups and snapshots are stored in S3.
                - you get free storage the size of your databse.
                - storage I/O may be suspended for a few seconds.
            - restoring an RDS database
                - The restored version of the database will always be a new RDS instance with a new DNS endpoint.
            - Encryption on RDS
                - this can be enable in creation time.
                - It's also integrated wiht Key mAnagement Service(KMS)
                - AES-256 encryption.
                - Include all DB Storage.
                - Encryption cannot be enabled when an RDS instance has been created.
                - the only way to encrypt in this case is by using your RDS snapshot.
        - Elasticcache
            - This is in memory Cache(key vlue)
            - improves Database performance.
                - allows you retrive information faster.
            - great for read-heavy Database workloads
                - caching the results of I/O intensive database queries.
            - The 2 types of Elasticcache
                - memcached, great for basic object caching, scales horizontally but is no Multi-AZ or failover.
                - good choice for basic caching.
                - Redis is a more sophisticated solution with enterprise features lik persistence, replication, Multi-AZ and failover.
                - it supports sorting and ranking data(gaming leaderboards) and complex data types likes lists and hashes.
                - So in case Elasticache is a good choice if you database is read-heavy and not prone to frequent changes.
                - also elasticache is not great solution if your Database have heavy write loads.
                - This also not help with Analytical processing(OLAP queries).
        - Parameter Store
            - This allows you to store licence keys, database connection information, username and passwords.
            - Thsi service under AWs Systsem Manager.
            - Parameter Store has 2 tiers
                - Standard
                - Advance
            - You can use KMS to encrypt your strings.
            - you also reference your parameters an example of this will be a bootstrap script.
            - This service can be used with CloudFormation, Ec2, Lambda, CodeBuild, Codepipeline and CodeDeploy.
        - quick note
            -RDS is not suitable for Analytical workloads but we can use RedShift for that.
- S3 Bucket section for Developer
    - S3 is an object storage that is secure, highly scable.
    - It also give you unlimited storage.
    - objects can range to o bytes to a maximum of 5TB
    - stores data in buckets.
    - when the upload is successful you get status 200 HTTP
    - S3 is key-value store
        - key as the name of the object.
        - this is the data itself.
        - version ID which allows us to store multiple objects.
        - Metadata which is data about data.
    - S3 is highly available and highly durable
        - build for availability.
        - is designed for durability.
    - S3 has characteristics
        - tiered Storage.
        - Lifecycle Management.
        - Versioning.
    - In terms of securing data
        - server-side encryption.
            - you can setup defualt encryption on a bucket.
        - Access control Lists(ACLs)
            - it define which AWS accounts or groups are granted access or types of access.
        - Bucket policies
            - this specify what actions are allowed or denied(PUT, DELETE).
        - 
- Severless
    - Serverless allows you to run your application code in the cloud without having to worry about managing any servers.
    - AWS handles
        - Capaciity provisioning.
        - paching.
        - auto scaling.
        - high availability.
    - Serverless Technologies on AWS
        - Lambda.
        - SQS.
        - SNS.
        - API Gateway.
        - DynamoDB.
    - Lambda
        - This is a serverless compute,it allows you to run your code on AWS without provisoning a server .
        - it takes care of everything including the runtime environment.
        - The supported the languages, Java, Go, Powershell, Node.js, C#, Python and Ruby.
        - This service including auto-scaling and high availability.
        - based on pricing you're charged based on number of requests, their duration, and the amount of memory.
        - The first 1 million requests per month are free.
        - duration, you're charged millisecond increments and the price depends on the memory you allocate.
        - price per GB-second 0.5GB x 0.1s = 0.5 GB-second.
        - you also get 400 000 GB-second per month from AWS for free.
        - Lambda is event-driven Architecture
            - this means they can be automatically trigged by other AWS services.
            - it is trigged by events, S3 or DynamoDB table changes.
            - it can be trigged by user request, this can be API Gateway config for HTTP endpoint.
        - Services that can trigger or invoke Lambda functions.
            - DynamoDB.
            - Kinesis.
            - SQS.
            - Aplication Load Balancer.
            - API Gateway.
            - Axela.
            - CouldFront.
            - S3.
            - SNS.
            - SES.
            - couldFormation.
            - CloudWatch.
            - CodeCommit.
            - CodePipeline.
    - API Gateway
        - Application programming interface and it is used to interact with other web applications.
        - This application allows you to publish, maintain, monitor and secure APIs at any scale.
        - Supported APIs
            - Restful APIs - severless workloads, stateless.
            - Websocket APIs - chat apps.
        - API Gateway can offer it's to Lambda, EC2 and DynamoDB.
    - Lambda versions
        - multiple versions can be created with different aliases.
        - $LATEST, we use arn:aws to track versions with Lambda.
    - to create versions you have to update one with $LASTED.
    - after creating a version we going to need to create an alias.
    - once a version is created the will update automatically with just publishing the new version.
    - but aliases need to be created manually.
    - lambda concurrent execution limit.
    - Lambda and VPCs
        - In some cases you will need to require to access resourcs in a private vpc, it can be read, write or RDS database access.
        - or shutdown EC2 insrtances.
        - the lambda function can be configured under advance settings for vpc options.
    - Step Functions
        - this privides a visual interface for serverless applications.
            - sequential workflows.
            - parallel workflows.
            - branching workflows.
        - you can log all your steps.
        - Standard Woorkflows
            - Long running up for a year.
            - at most once model, tasks are never excuted more than once.
            - non idempotent actions, when processing payments.
            - change in state.
        - Express Workflows
            - short-lived.
            - at least once.
            - idempotent.
            - identical request.
        - 2 Types of Express workflows
            - Synchronous express workflow.
                - begins a workflow and waits for it to complete.
            - Asynchronous express workflow.
                - begins a workflow and confirms the workflow and you can just check the logs on CloudWatch.
    - X-Ray
        - is a tool which helps developers analyze and debug distributed applications.
        - allwing you to toubleshoot the root cuase of performance issues and erros.
        - Analyze and debug
            - analyze and debug distributed applications.
        - Service map
            - visual presentation of applications.
        - X-Ray Agent and X-Ray SDK
            - it's agent/daemon that must be installed on your EC2 instance or use SDK to instrument your application to send traces to X-Ray.
        - X-Ray integrations
            - EC2.
            - Elastic Container Service (ECS).
            - Lambda.
            - Elastic Beanstalk.
            - SNS.
            - SQS.
            - DynomoDB
            - Elastic Load Balancer.
            - API Gateway.
        - X-Ray high level configuration ststeps
            - you need X-Ray SDK.
            - X-Ray daemon.
        - Annotations & Indexing
            - annotations.
            - key-value pairs.
    - Advance API Gateway.
        - import API with a definition files.
        - OpenAPI known as swagger
        - SOAP legacy protocols which returns a response in XML.
    - API Gateway caching & throttling
        - they are designed to improve performance with latency.
        - TTL (time to live) they're cached for this time period, the default TTL is 3000 seconds(5 minutes).
        - Reduce the number of API calls.
        - throttling help with reducing and manaing requst from your API.
        - defualt limts 10000 rps and 5000 concurrent requests.
        - you can import APIs using definition files like OPenAPI known as swagger.
- DynamoDb
    - This is a no SQL Database, it is fast and flexible, consistent, single digit millisecond latency at any scale.
    - fully managed, it supports key-value data models, also doument format such as JSON, HTML and XML.
    - Use case, is great fit for mobile, web, gaming, ad tech and Iot.
    - it is serverless which means it integrates well with Lambda.
    - storage is SSD.
    - eventually consistent reads.
    - Strongly consistent reads.
    - ACID transactions 
    - Primary key
        - partition key is input to an internal hash function or a physical location which the data is stored.
        - composite key is both partition key plus sort key(a timestap for a pos for example).
    - DynamoDB Access Control
        - We use IAM for permission access for DynamoDB to make make sure we can add condition which we add an IAM policy to restrict user for accessing other users data.
        - IAM dynamodb:leadingKeys which allows only the items where partion key-value maches their User_ID.
    - DynamoDB Indexing
        - querying in dynamodb on non-primary key and using global secondary indexes and local secondary indexes.
    - query finds items in a table
    - results sort key always sort in aascending order.
    - reverse set ScanIndexForward this reverse your order of your query and is not used for scans only quaries.
    - a query is more efficient than a scan.
    - DynamoDB API calls
        - create-table.
        - put-item.
        - get-item.
        - update-table.
        - list-table.
        - describe-table.
        - scan.
        - query.
        - delete-item.
        - delete-table.
    - IAM permissions are needed to make API calls.
    - DynamoDB Provisioned Throughput
        - when you write your table, you can specify you requirements.
        - calculating strongly read capacity 3KB/4KB = 0.75 0.75KB~ 1KB * 80 = 80.
        - for eventually consistence read you will divide 80/ 2 = 40 for eventually consistence reads.
        - write capacity each size item is 1KB 512 bytes / 1024 bytes = 0,5~ 1KB.
        - 1KB * 100 = 100 capacity unit.
    - DynamoDB on-demand Capacity Model
        - charges apply for reading, writing and sorting data.
        - DynamoDB instantly scales up and down.
        - we use this for unknown workloads.
        - spiky, short-lived peaks. a pay-per use model is desired
    - DynamoDB Accelerator (DAX)
        - it's a fully managed cluster in memory cache for DynamoDB
        - This allow to cache read permissions so it will quey Dax first in terms of reading the cached read on the table.
        - So Dax is not a benefit for very intensive read and write applications.
    - DynaoDB Time To Live (TTL)
        - this is for expiry time for data and it's great for remove irrelevent/old data and this reduces costs as well.
        - we use epoch time to estemate how much time it left to remove old data.
    - DynamoDB Streams
        - This ca be used for auditing and as trigger in terms of monitoring events on your DynamoDB table and is good for Serverless services such as Lambda.
        - time ordered sequence.
        - logs.
        - by default primary key is recorded.
        - it replicates the data and it doesn't store it for long.
    - Provisioned throughput exceeded
        - your request rate is too high for read/write capacity.
        - The AWS SDK will run the request until sucessful.
        - if you dont use the SDK, you can use the exponational backoff to reduce the frequency of requests.
        - exponational backoff
            - this improves flow by retrying requsts using progressively longer waists.
-  Key Management Service (KMS)
    - This allows you to manage all your enrypted keys in AWS.
    - you use it when dealing with sensitive information.
    - KMS integrates
        - S3.
        - RDS.
        - DynamoDB.
        - Lambda.
        - EBS.
        - EFS.
        - CloudTrail.
        - Developer Tools.
    - Customer Master Key (CMK)
        - Is used to Generate / Encrypt / decrypt the data key and the data key is used to encrypt / decrypt yor data and it can do this up to 4KB.
        - known as Envelope Encrypt.
        - to setup CMK
            - Set uo CMK, Alias -> Description -> Key material.
            - key Administration permissions, USers -> Roles -> admin permissions.
            - Key usage permissions, users -> roles.
    - KMS API Calls
        - kms encrypt.
        - kms decrypt.
        - kms re-encrypt.
        - kms enable-key-rotation.
        - kms generate-data-key > 4KB.
    - Envolope Encryption
        - it encrypts files that are larger than 4KB.
        - we can aslo decrypt as well.
        - we use this because the encryption goes through Network and also for performance.
    - KMS Components
        - AWS-managed CMK.
        - Customer-managed CMK.
        - Data key, known as envolope encryption.
- Simple Queue Service (SQS)
    - it is a message queue serivice.
    - This nables web service applications to queue messages.
    - a way of assigning jobs in a form of messages.
    - The visibility timeout takes place when the process is running.
    - SQS features
        - Decouple application Components
        - Store messages.
        - retrieve Messages.
    - SQS acts as a buffer between  the component receiving the data processing and producing / saving the data.
    - Resolves scheduling issues
    - SQS Key facts
        - Pull-based
        - text data.
        - 256 KB messages in size.
        - messages will be processed at least once.
        - messages can be kept on the queue for 1 minute to 14 days.
        - defualt retention is 4 days.
    - SQS types
        - standard queues are the default which provide best-effort ordering.
            - unlimited transactions.
            - best effort ordering.
        - FIFO (first in first out).
            - exactly once processing
            - no duplicates.
            - 300 tps limit.
            - great option for banks.
    - SQS Settings
        - visibility timeout
            - it is the amount of time the message is invisible in the SQS queue after a redaer picks up that message.
            - by default is 30 seconds.
            - we can change the visibility which is 12 hours.
            - short polling and long pulling.
    - SQS delay queues
        - postpone delivery of new messages
        - we use this for large distributed applications
        - delay for a fiew seconds.
        - We use S3 for large SQS messages 256KB up to 2GB.
        - We need AWS SDK for java, SQS extended client library for java and S3 bucket.
        - We can't use AWS CLI, AWS Management console, SQS API and any other AWS SDK.
- Simple Notification Service (SNS)
    - A web service that makes it easy to set up, operate and send notifications from the Cloud.
    - it supports sms, SQS, HTTP and email.
    - It's a push notification only.
    - it uses publish and subscribe model so the customer needs to subcribe.
    - it can fanout mssages to a large number of Users, to multiple of SQS queues, HTTP endpoints and emails.
- Simple Email Service
    - it is a an email only service.
    - this can be use for incoming and outgoing emails.
    - it is not subcribe based, you just need he email.

- Kinesis
    - We have Kinesis streams which captures streaming of videos and data in real time and applications that process and analyse data in real time.
    - Kinesis Data FireHose capture, transform and load data continuously into AWS data stores for example DataDogs, Splunk or using BI applications and tools that can be real time analysis for stored data.
    - Kinesis Data Analytics
        - a real time analytics using standard SQL on data received by Kinesis Data Streams and Kinesis Data Firehose, processing data in AWS data store, S3, Redshift, Elasticsearch.
    - Kinesis streams are made up of shards
        - each shard it's a sequence of or more data records and provides a fixed unit of Capacity.
        - Consuming keinesis the number of consumer instances shuold not exceed the number  of shards (except for failover purposes).
        - Will need to use auto-scaling group and drive scaling actions for your EC2 consumer instances based on the CPU utilization.
- Elastic beanstalk
    - deploys ans scales your web applications, including web application server platform.
    - languages supported is Java, PHP, Python, GO, .NET, Node.js.
    - managed platforms like Apache Tomcat, Docker.
    - it also provisions AWS resources like:
        - EC2.
        - RDS.
        - S3.
        - Elastic Load Balancer.
        - Auto Scaling Groups.
    - System Admin , managing OS and applications server updates, monitoring, metrics and health checks.
    - it can also can fully manage your EC2 onstances for you or you can take full administraive control.
    - You can Customize your Elastic Beanstalk.
        - env with .ebextensions folder located in top-level directory and the files must have .config this is for AMAzong linux1.
        - for Amazon linux2 we can use a build file in the root directory of your application like shell scripts.
        - we can create procfile for long-ryunning process like custom commands.
        - platform hooks for custom scripts that run various stages when the EC2 instance are provisioned
            - .platform / hooks / prebuild / predeploy / postdeploy.
    - Windows Application Migration Assistant for Elastic Beanstalk
        - it's formerly known as .NET Migration Assistant
        - open source, interactive Powershell migrating windows resource to elastic beanstalk
    - Elastic beanstalk deployment types
        - all at once.
        - rolling update deployed in baches.
        - rolling with additional batch, maintains full capacity.
        - immutable maintains full capacity and to roll back you just delete the new instance.
        - Traffic splitting which performs an immutable deployment the splits the triffc between the old and the new deployment.
    - Deploying with RDS with Elastic beanstalk
- Developer Tools
    - CodeCommit
        - source and version control.
        - this service allows Devs to collaborate on projeects.
    - CodeBuild
        - Automated builds.
        - compiles source code.
    - CodeDeploy
        - automated deployment.
        - we use it to automate code deployment to EC2 instances, lambda and on-premises systems.
    - CodePipeline
        - manages the workflow
        - end to end solution, allowing you to build, test, and deploy your application everytime the is a code change.
    - CI/CD
        - integrating or merging the code changes triggerd by CodeCommit.
        - automating builds for CD trigrred by CodeBuild and CodeDeploy.
        - continuous Deployment fully automating release process so this is part of Codepipeline.
    - CodeArtifact
        - it is a repository that makes it easy for developers to find software packages
        - packges which are needed for developemnt and they're approved and we can create an upstream repo with external connections to pull packages npm for example.
    - 


        


